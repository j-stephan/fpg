\section{Messungen auf AMD-GPUs}
\label{amd}

\subsection{Verwendete Hard- und Software}

Die hier gezeigten Benchmark-Ergebnisse wurden auf einem Rechner mit der
folgenden Hardware gemessen:

\begin{itemize}
    \item CPU: AMD Ryzen Threadripper 1950X
          \begin{itemize}
              \item \num{16} Kerne
              \item \num{32} virtuelle Kerne
              \item maximaler Takt: \SI{3,4}{\giga\hertz}
          \end{itemize}
    \item GPU: AMD Radeon RX Vega 64
          \begin{itemize}
              \item \num{64} Multiprozessoren
              \item \num{64} Kerne pro Multiprozessor (insgesamt \num{4096}
                    Kerne)
              \item maximaler Takt: \SI{1536}{\mega\hertz}
              \item \SI{8}{\gibi\byte} HBM2-Speicher
              \item Speicherbusbreite: \SI{2048}{\bit}
              \item Speicherbandbreite: \SI{483,3}{\gibi\byte\per\second}
              \item Speichertakt: \SI{945}{\mega\hertz}
              \item kein ECC
          \end{itemize}
      \item RAM: \SI{64}{\gibi\byte}
\end{itemize}

Das verwendete Betriebssystem war Ubuntu 16.04 mit der Linux-Kernel-Version
4.15. Für die \gls{gpgpu}-Programmierung kamen die mit der \gls{rocm}-Version
2.1.96 mitgelieferten HIP- und HC-Implementierungen zum Einsatz.

\subsection{zcopy}

\subsubsection{Vorüberlegungen}

Die in Abschnitt~\ref{nvidia:zcopy:vorueberlegungen} für NVIDIA-GPUs gemachten
Überlegungen können nicht ohne Anpassungen für AMD-GPUs übernommen werden, da
sich die Multiprozessor-Architekturen unterscheiden.

Jeder Multiprozessor einer Vega-GPU besteht aus vier \gls{simd}-Einheiten. Eine
\gls{simd}-Einheit verfügt über 16 Vektor-ALUs (VALUs), die synchron auf jeweils
einem \textit{work-item} arbeiten. Die vier \gls{simd}-Einheiten arbeiten
parallel eine \textit{wavefront} ab, die 64 \textit{work-items} umfasst.

Anders ausgedrückt kann jeder Multiprozessor genau 64 Threads abarbeiten, wobei
diese auf dem Multiprozessor noch einmal in Gruppen von 16 Threads aufgeteilt
werden. Ausgehend von NVIDIAs \textit{half-warps} könnte man diese Gruppen auch
als \textit{quarter-wavefronts} bezeichnen, wenngleich dieser Begriff offiziell
nicht existiert.

Die Multiprozessoren verfügen über einen L1-Cache, der eine Cacheline-Größe von
64 Byte aufweist. Um jede \gls{simd}-Einheit effizient zu befüllen, muss jeder
Thread genau 4 Bytes lesen, was einem \texttt{float} entspräche. Um den
Speicher-Controller besser auszulasten, wird dieser Wert auf 8 Byte pro Thread
erhöht (Datentyp \texttt{hc::short\_vector::float\_2}). Die transportierte
Datenmenge pro Wavefront beträgt somit $8 \cdot 64 = 512$ Bytes und entspricht
damit der der CUDA-Variante ($16 \cdot 32 = 512$), bei der ein
\texttt{float4}-Vektor verwendet wurde.

\subsubsection{Messmethoden}
\label{amd:zcopy:methoden}

Die Messmethoden entsprechen dem in Abschnitt~\ref{nvidia:zcopy:methoden}
für NVIDIA-GPUs geschilderten Vorgehen.

Die im Quelltext~\ref{amd:zcopy:befehle} zeigen die verwendeten Compiler-Flags
sowie die Festsetzung der Multiprozessor-Taktrate.

\begin{code}
    \begin{minted}[fontsize=\small]{bash}
# HC-Compiler
hcc `hcc-config --cxxflags --ldflags` -O3 -std=c++17 \
  -amdgpu-target=gfx900

# HIP-Compiler (-amdgpu-target wird nicht unterstützt)
hipcc -O3 -std=c++17

# Taktrate
rocm-smi --setsclk 7
    \end{minted}
    \caption{Compiler-Flags und Taktrate für zcopy}
    \label{amd:zcopy:befehle}
\end{code}

\subsubsection{Ergebnisse}

Wie das Ergebnis für den kombinierten Lese- und Schreibvorgang zeigt (siehe
Abbildung~\ref{amd:zcopy:rw}), stagniert die Bandbreite der \gls{hc}-Kernel mit
steigender Tile-Zahl. Auch scheint eine mittlere Anzahl an Threads pro Tile
vorteilhaft sein.

Ein interessanter Effekt ist für den reinen Schreibvorgang zu beobachten (siehe
Abbildung~\ref{amd:zcopy:w}): Während Tiles mit \num{64} oder \num{256} Threads
eine nahezu konstante Bandbreite ermöglichen, brechen alle anderen Größen stark
ein, bevor sie sich mit steigender Tile-Zahl wieder in die Richtung des Maximums
bewegen. Eine Erklärung lässt sich dafür nur schwer finden, da es keine auf den
ersten Blick schlüssige Begründung für das Verhalten der \num{128}er-Tiles gibt,
die sich mit dem Verhalten der großen Tiles in Einklang bringen lässt. Denkbar
ist ein Fehler im Scheduler oder eine in dieser Konfiguration schlechte
Auslastung des Speichercontrollers; hier wären weitergehende Untersuchungen
erforderlich.

Die \gls{hip}-Variante verhält sich wie die \gls{hc}-Implementierung, die
Ergebnisse sind dieser Arbeit in Abschnitt~\ref{anhang:hip:amdzcopyfig}
angehängt.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Vega 64 -- zcopy -- Lesen + Schreiben -- HC},
            xlabel = {Tiles pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 4096,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-64,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-rw.csv};
            \addlegendentry{$\text{Tile-Größe} = 64$} 

            \addplot table [x = blocks_per_sm, y = throughput-128,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-rw.csv};
            \addlegendentry{$\text{Tile-Größe} = 128$} 

            \addplot table [x = blocks_per_sm, y = throughput-256,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-rw.csv};
            \addlegendentry{$\text{Tile-Größe} = 256$} 

            \addplot table [x = blocks_per_sm, y = throughput-512,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-rw.csv};
            \addlegendentry{$\text{Tile-Größe} = 512$} 

            \addplot table [x = blocks_per_sm, y = throughput-1024,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-rw.csv};
            \addlegendentry{$\text{Tile-Größe} = 1024$} 
        \end{axis}
    \end{tikzpicture}
    \caption{zcopy: Bandbreite der Vega 64 (Lesen und Schreiben, HC)}
    \label{amd:zcopy:rw}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Vega 64 -- zcopy -- Schreiben -- HC},
            xlabel = {Tiles pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 4096,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-64,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-w.csv};
            \addlegendentry{$\text{Tile-Größe} = 64$} 

            \addplot table [x = blocks_per_sm, y = throughput-128,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-w.csv};
            \addlegendentry{$\text{Tile-Größe} = 128$} 

            \addplot table [x = blocks_per_sm, y = throughput-256,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-w.csv};
            \addlegendentry{$\text{Tile-Größe} = 256$} 

            \addplot table [x = blocks_per_sm, y = throughput-512,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-w.csv};
            \addlegendentry{$\text{Tile-Größe} = 512$} 

            \addplot table [x = blocks_per_sm, y = throughput-1024,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-w.csv};
            \addlegendentry{$\text{Tile-Größe} = 1024$} 
        \end{axis}
    \end{tikzpicture}
    \caption{zcopy: Bandbreite der Vega 64 (Schreiben, HC)}
    \label{amd:zcopy:w}
\end{figure}

Ein Leistungsunterschied besteht in der erreichten Bandbreite: \gls{hip} ist
hier messbar schneller, wenn auch nur um wenige \si{\giga\byte\per\second}. Dies
ist sowohl für den kombinierten Lese- und Schreib-Kernel als auch den reinen
Schreibvorgang der Fall, wie die Abbildungen~\ref{amd:zcopy:vergleichrw} und
\ref{amd:zcopy:vergleichw} zeigen.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Vega 64 -- zcopy -- Lesen + Schreiben},
            xlabel = {Tiles pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 4096,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-256,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-rw.csv};
            \addlegendentry{HC} 

            \addplot table [x = blocks_per_sm, y = throughput-256,
                            col sep = semicolon]
                           {data/zcopy-amd-hip-vega64-rw.csv};
            \addlegendentry{HIP} 
        \end{axis}
    \end{tikzpicture}
    \caption{zcopy: Bandbreite der Vega 64 (256er-Blöcke, Lesen+Schreiben)}
    \label{amd:zcopy:vergleichrw}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Vega 64 -- zcopy -- Schreiben},
            xlabel = {Tiles pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 4096,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-256,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-w.csv};
            \addlegendentry{HC} 

            \addplot table [x = blocks_per_sm, y = throughput-256,
                            col sep = semicolon]
                           {data/zcopy-amd-hip-vega64-w.csv};
            \addlegendentry{HIP} 
        \end{axis}
    \end{tikzpicture}
    \caption{zcopy: Bandbreite der Vega 64 (256er-Blöcke, Schreiben)}
    \label{amd:zcopy:vergleichw}
\end{figure}

Wie die Abbildung~\ref{amd:zcopy:peak} zeigt, lassen sich etwa 90\% der
theoretisch möglichen Bandbreite auch praktisch nutzen.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Vega 64 -- zcopy -- HC},
            xlabel = {Tiles pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 4096,
            ymin = 180, ymax = 500,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            extra y ticks = 483.8,
            extra y tick labels = {$483.8$},
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = blocks_per_sm, y = throughput-256,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-rw.csv};
            \addlegendentry{Lesen+Schreiben} 

            \addplot table [x = blocks_per_sm, y = throughput-256,
                            col sep = semicolon]
                           {data/zcopy-amd-hc-vega64-w.csv};
            \addlegendentry{Schreiben} 

            \addplot table [x = blocks_per_sm, y = peak, col sep = semicolon]
                           {data/zcopy-amd-vega64-peak.csv};
            \addlegendentry{Peak}
        \end{axis}
    \end{tikzpicture}
    \caption{zcopy: Theoretische und praktische Bandbreite der Vega 64 (HC,
             256er-Tiles)}
    \label{amd:zcopy:peak}
\end{figure}

\subsection{Reduction}

\subsubsection{Implementierung}

Die Implementierungen des Reduktionskernels sind dieser Arbeit angehängt und
befinden sich in den Quelltexten~\ref{anhang:hc:reduction} (\gls{hc}) und
\ref{anhang:hip:reduction} (\gls{hip}).

\subsubsection{Messmethoden}

Die Messmethoden entsprechen den in Abschnitt~\ref{nvidia:reduction:methoden}
für die NVIDIA-Implementierungen geschilderten, die Compiler-Flags und
GPU-Einstellungen sind dieselben wie für den zcopy-Benchmark (siehe
Abschnitt~\ref{amd:zcopy:methoden}).

\subsubsection{Ergebnisse}

Wie in der Abbildung~\ref{amd:reduction:hc} zu sehen ist, sind 256 Threads die
beste Tile-Größe der HC-Implementierung, während zwei Tiles dieser Größe die
beste Anzahl pro Multiprozessor darstellen. Dies gilt auch für HIP (siehe
Abbildung~\ref{anhang:amd:reduction:hip}).

Im direkten Vergleich der Spracherweiterungen wird sichtbar, dass sich die
zcopy-Ergebnisse bei der Reduktion bestätigen lassen. Wieder erreichen \gls{hc}
und \gls{hip} eine ähnliche Bandbreite, wobei \gls{hip} konstant und messbar
bessere Werte erreicht. Beide Implementierungen kommen außerdem sehr nah an die
tatsächlich erreichbare Bandbreite heran, die im zcopy-Benchmark ermittelt wurde
(siehe Abbildung~\ref{amd:reduction:peak}).

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Vega 64 -- Reduction -- HC},
            xlabel = {Tiles pro Multiprozessor},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 2, xmax = 1024,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = tiles, y = dim64,
                            col sep = semicolon]
                           {data/reduce-amd-hc-vega64.csv};
            \addlegendentry{Tile-Größe = 64} 

            \addplot table [x = tiles, y = dim128,
                            col sep = semicolon]
                           {data/reduce-amd-hc-vega64.csv};
            \addlegendentry{Tile-Größe = 128} 

            \addplot table [x = tiles, y = dim256,
                            col sep = semicolon]
                           {data/reduce-amd-hc-vega64.csv};
            \addlegendentry{Tile-Größe = 256} 

            \addplot table [x = tiles, y = dim512,
                            col sep = semicolon]
                           {data/reduce-amd-hc-vega64.csv};
            \addlegendentry{Tile-Größe = 512} 

            \addplot table [x = tiles, y = dim1024,
                            col sep = semicolon]
                           {data/reduce-amd-hc-vega64.csv};
            \addlegendentry{Tile-Größe = 1024} 
        \end{axis}
    \end{tikzpicture}
    \caption{Reduction: Bandbreite der Vega 64 (HC)}
    \label{amd:reduction:hc}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Vega 64 -- Reduction},
            xlabel = {$n$},
            ylabel = {Bandbreite [\si{\gibi\byte\per\second}]},
            xmode = log,
            log basis x = 2,
            xmin = 65536, xmax = 134217728,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = south east,
            no markers,
            every axis plot/.append style = {very thick},
            width = 0.75\textwidth,
            scale only axis,
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = n, y = HC,
                            col sep = semicolon]
                           {data/reduce-amd-vega64.csv};
            \addlegendentry{HC} 

            \addplot table [x = n, y = HIP,
                            col sep = semicolon]
                           {data/reduce-amd-vega64.csv};
            \addlegendentry{HIP} 

            \addplot table [x = , y = Peak, col sep = semicolon]
                           {data/reduce-amd-vega64.csv};
            \addlegendentry{Peak}
        \end{axis}
    \end{tikzpicture}
    \caption{Reduction: Bandbreite der Vega 64 (zwei 256er-Tiles pro
             Multiprozessor)}
    \label{amd:reduction:peak}
\end{figure}

\subsection{N-Body}

\subsubsection{Implementierung}

Die Implementierung für \gls{hc} und \gls{hip} folgen denselben Prinzipien, die
für die NVIDIA-Implementierungen in Abschnitt~\ref{nvidia:nbody:implementierung}
aufgeführt wurden.

\subsubsection{Messmethoden}

Auch die Messmethoden entsprechen denen für NVIDIA-GPUs (siehe
Abschnitt~\ref{nvidia:nbody:methoden}). Die Compiler-Flags und GPU-Einstellungen
sind mit denen für den zcopy-Benchmark identisch (siehe
Abschnitt~\ref{amd:zcopy:methoden}).

\subsubsection{Optimierung und Auswertung}

Der durch das Ausrollen der Schleifen erzielte Effekt ist deutlich in den
Abbildungen~\ref{amd:nbody:unroll-hc} und \ref{anhang:amd:nbody:unroll-hip} zu
sehen: durch die Bestimmung eines besseren Ausrollfaktors lassen sich in diesem
Benchmark bei einer festen Kachelgröße von $p = 256$ knapp \num{1000} GFLOPS
mehr Durchsatz gewinnen. Anhand dieser Messung wurde für den weiteren Verlauf
ein Ausrollfaktor von \num{8} festgelegt.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Vega 64 -- N-Body -- Ausrollen -- HC},
            xlabel = {Ausrollfaktor},
            ylabel = {GFLOPS},
            xmode = log,
            log basis x = 2,
            xmin = 1, xmax = 512,
            xtick = {1,2,4,8,16,32,64,128,256,512},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = outer north east,
            no markers,
            every axis plot/.append style = {very thick},
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = count, y = gflops-hc, col sep = semicolon]
                           {data/nbody-amd-unroll-524288.csv};
            \addlegendentry{$n = 524.288$} 

            \addplot table [x = count, y = gflops-hc, col sep = semicolon]
                           {data/nbody-amd-unroll-65536.csv};
            \addlegendentry{$n = 65.536$} 

            \addplot table [x = count, y = gflops-hc, col sep = semicolon]
                           {data/nbody-amd-unroll-8192.csv};
            \addlegendentry{$n = 8.192$} 
        \end{axis}
    \end{tikzpicture}
    \caption{N-Body: Performanzgewinn durch das Ausrollen der Schleife (HC)}
    \label{amd:nbody:unroll-hc}
\end{figure}

Der nächste performanzrelevante Faktor ist die Größe der Kacheln selbst. Aus den
in den Abbildungen~\ref{amd:nbody:tilesize-hc} und
\ref{anhang:amd:nbody:tilesize-hip} dargestellten Messergebnissen wird
ersichtlich, dass die Kachelgröße für den Benchmark weniger wichtig ist;
relevante Unterschiede sind nur bei großen Kachelgrößen und wenigen Elementen
sichtbar. Das ist ein wesentlicher Unterschied zu den in
Abschnitt~\ref{nvidia:nbody:auswertung} für NVIDIA-GPUs gemessenen Ergebnissen,
die von der Kachelgröße stark beeinflusst wurden. Für den weiteren Messverlauf
wird daher eine Kachelgröße von $p = 256$ angenommen.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Vega 64 -- N-Body -- Kachelgrößen -- HC},
            xlabel = {Kachelgröße},
            ylabel = {GFLOPS},
            xmode = log,
            log basis x = 2,
            xtick = {64,128,256,512,1024},
            xticklabel = {\xinttheiexpr2^\tick\relax},
            log ticks with fixed point,
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = outer north east,
            no markers,
            every axis plot/.append style = {very thick},
            cycle list name = exotic,
            /pgf/number format/.cd, use comma
        ]
            \addplot table [x = tilesize, y = gflops-hc, col sep = semicolon]
                           {data/nbody-amd-tilesize-524288.csv};
            \addlegendentry{$n = 524.288$} 

            \addplot table [x = tilesize, y = gflops-hc, col sep = semicolon]
                           {data/nbody-amd-tilesize-65536.csv};
            \addlegendentry{$n = 65.536$} 

            \addplot table [x = tilesize, y = gflops-hc, col sep = semicolon]
                           {data/nbody-amd-tilesize-8192.csv};
            \addlegendentry{$n = 8.192$} 
        \end{axis}
    \end{tikzpicture}
    \caption{N-Body: Performanz bei verschiedenen Kachelgrößen (HC)}
    \label{amd:nbody:tilesize-hc}
\end{figure}

Mit der experimentell ermittelten Konfiguration lässt sich ein direkter
Vergleich zwischen \gls{hc} und \gls{hip} anstellen. Die
Abbildung~\ref{amd:nbody:comparison} zeigt, dass die Performanz bei beiden
\gls{api}s nahezu identisch ist. Der Blick in den generierten Maschinen-Code
zeigt, dass der \texttt{hcc}-Compiler in der Lage ist, für beide Varianten ein
identisches Ergebnis zu erzeugen (siehe Quelltexte~\ref{amd:nbody:isahc} und
\ref{amd:nbody:isahip}).

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title = {Vega 64 -- N-Body -- Leistungsvergleich -- HC und HIP},
            xlabel = {$n$},
            ylabel = {GFLOPS},
            xtick = data,
            xmode = log,
            log basis x = 2,
            xticklabel = {\xinttheiexpr2^\tick\relax},
            ymajorgrids = true,
            xmajorgrids = true,
            grid style = dashed,
            legend cell align = left,
            legend pos = outer north east,
            no markers,
            every axis plot/.append style = {very thick},
            cycle list name = exotic,
            /pgf/number format/.cd, use comma,
            ybar,
            width = 0.75\textwidth,
            scale only axis,
            ymin = 0, ymax = 13000,
            extra y ticks = 12583,
            extra y tick labels = {},
            extra y tick style={grid=major, major grid style={solid,thick,draw=red}},
            scaled y ticks = false,
            ylabel near ticks,
            xlabel near ticks
        ]
            \addplot table [x = n, y = gflops-hc, col sep = semicolon]
                           {data/nbody-amd.csv};
            \addlegendentry{HC} 

            \addplot table [x = n, y = gflops-hip, col sep = semicolon]
                           {data/nbody-amd.csv};
            \addlegendentry{HIP} 

            \addlegendimage{peak}
            \addlegendentry{Max.}
        \end{axis}
    \end{tikzpicture}
    \caption{N-Body: Leistungsvergleich zwischen HC und HIP}
    \label{amd:nbody:comparison}
\end{figure}

\begin{figure}
    \begin{minipage}{0.5\textwidth}
        \centering
        \begin{minted}[fontsize=\footnotesize]{text}
ds_read2_b64 v[14:17], v10 offset1:1
v_add_u32_e32 v9, 64, v9
s_waitcnt lgkmcnt(0)
v_sub_f32_e32 v16, v16, v5
v_sub_f32_e32 v15, v15, v4
v_fma_f32 v18, v16, v16, s20
v_sub_f32_e32 v14, v14, v3
v_fma_f32 v18, v15, v15, v18
v_fma_f32 v18, v14, v14, v18
v_mul_f32_e32 v19, v18, v18
v_mul_f32_e32 v18, v18, v19
v_cmp_gt_f32_e32 vcc, s21, v18
v_mov_b32_e32 v19, s22
v_cndmask_b32_e32 v20, 1.0, v19, vcc
v_mul_f32_e32 v18, v18, v20
v_rsq_f32_e32 v18, v18
v_mov_b32_e32 v20, s23
v_cndmask_b32_e32 v21, 1.0, v20, vcc
v_mul_f32_e32 v18, v21, v18
v_mul_f32_e32 v17, v17, v18
v_fma_f32 v18, v14, v17, v11
v_fma_f32 v15, v15, v17, v12
v_fma_f32 v16, v16, v17, v13                   
        \end{minted}
        \captionof{listing}{N-Body: Maschinencode des HC-Kernels}
        \label{amd:nbody:isahc}
    \end{minipage}
    %
    \begin{minipage}{0.5\textwidth}
        \centering
        \begin{minted}[fontsize=\footnotesize]{text}
ds_read2_b64 v[14:17], v9 offset1:1
v_add_u32_e32 v10, 64, v10
s_waitcnt lgkmcnt(0)
v_sub_f32_e32 v16, v16, v5
v_sub_f32_e32 v15, v15, v4
v_fma_f32 v18, v16, v16, s16
v_sub_f32_e32 v14, v14, v3
v_fma_f32 v18, v15, v15, v18
v_fma_f32 v18, v14, v14, v18
v_mul_f32_e32 v19, v18, v18
v_mul_f32_e32 v18, v18, v19
v_cmp_gt_f32_e32 vcc, s17, v18
v_mov_b32_e32 v19, s18
v_cndmask_b32_e32 v20, 1.0, v19, vcc
v_mul_f32_e32 v18, v18, v20
v_rsq_f32_e32 v18, v18
v_mov_b32_e32 v20, s19
v_cndmask_b32_e32 v21, 1.0, v20, vcc
v_mul_f32_e32 v18, v21, v18
v_mul_f32_e32 v17, v17, v18
v_fma_f32 v18, v14, v17, v11
v_fma_f32 v15, v15, v17, v12
v_fma_f32 v16, v16, v17, v13
        \end{minted}
        \captionof{listing}{N-Body: Maschinencode des HIP-Kernels}
        \label{amd:nbody:isahip}
    \end{minipage}
\end{figure}

Beide \gls{api}s erreichen jedoch nicht einmal die Hälfte der theoretisch
möglichen FLOPS.
